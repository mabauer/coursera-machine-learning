{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Neural Networks Learning\n",
    "\n",
    "In this exercise, you will implement the backpropagation algorithm for neural networks and apply it to the task of hand-written digit recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dataset\n",
    "\n",
    "We are given a data set in `ex4data1.mat` that contains 5000 training examples of handwritten digits. (This is exactly the same data set as in last week's exercise 3).\n",
    "\n",
    "Each training example is a 20 pixel by 20 pixel grayscale image of the digit. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels is “unrolled” into a 400-dimensional vector. Each of these training examples becomes a single row in our data matrix X. This gives us a 5000 by 400 matrix X where every row is a training example for a handwritten digit image.\n",
    "\n",
    "The second part of the training set is a 5000-dimensional vector y that contains labels for the training set. Like in the last exercise, a “0” digit is labeled as “10”, while the digits “1” to “9” are labeled as “1” to “9” in their natural order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Load saved matrices for X and y from file\n",
    "load('data/ex4data1.mat');\n",
    "% The matrices X and y will now be in our Octave environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network is shown in the picture below. It has 3 layers – an input layer, a hidden layer and an output layer. Recall that our inputs are pixel values of digit images. Since the images are of size 20 × 20, this gives us 400 input layer units (not counting the extra bias unit which always outputs +1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Structure fo our Neural Network](network_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by visualizing a subset of the training set. We reuse the 'displayDat'a function from the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = size(X, 1);\n",
    "\n",
    "% Randomly select 100 data points \n",
    "rand_indices = randperm(m);\n",
    "sel = X(rand_indices(1:100), :);\n",
    "% and display them\n",
    "displayData(sel);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to get started, we have been provided with a set of network parameters $ (\\Theta^{(1)},\\Theta^{(2)}) $ that have neen previously trained. These are stored in ex4weights.mat. The parameters have dimensions that are sized for a neural network with 25 units in the second layer and 10 output units (corresponding to the 10 digit classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Load trained parameters as matrices Theta1 and Theta2 \n",
    "load('data/ex4weights.mat');\n",
    "% Theta1 has size 25 x 401\n",
    "% Theta2 has size 10 x 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400;  % 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;   % 25 hidden units\n",
    "num_labels = 10;          % 10 labels, from 1 to 10   \n",
    "                          % (note that we have mapped \"0\" to label 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation and cost function\n",
    "\n",
    "As a first step, we will implement the cost function and the gradient for the neural network. The cost function (without regularization) is defined as:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K}[-y_k^{(i)} \\log(h_\\theta(x^{(i)})_k) - (1-y_k^{(i)}) \\log(1 - h_\\theta(x^{(i)})_k) ] $$ \n",
    "\n",
    "where $ K $ is the number of labels (or outputs, in our case $ K = 10$ and $ h_\\theta(x^{(i)})_k = a^{(3)}_k $ is the activation value of the $k$th unit in the output layer (compare with neural network layout depicted above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always we make use of a couple of helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function g = sigmoid(z)\n",
    "%SIGMOID Compute sigmoid function\n",
    "%   g = SIGMOID(z) computes the sigmoid of z.\n",
    "\n",
    "    % The function should work on scalar *and* matrix values \n",
    "    g = zeros(size(z));\n",
    "    \n",
    "    g = 1 ./ ( 1 + exp(-z));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function g = sigmoidGradient(z)\n",
    "%SIGMOIDGRADIENT returns the gradient of the sigmoid function\n",
    "%evaluated at z\n",
    "%   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function\n",
    "%   evaluated at z. t.\n",
    "\n",
    "    % The function should work on scalar *and* matrix values\n",
    "    g = zeros(size(z));\n",
    "\n",
    "    g = sigmoid(z) .* (1-sigmoid(z));\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n",
      "  \n",
      "g =\n",
      "\n",
      "   0.19661   0.23500   0.25000   0.23500   0.19661\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "fprintf('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ');\n",
    "g = sigmoidGradient([-1 -0.5 0 0.5 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "function result = h(theta, x)\n",
    "    result = (sigmoid(theta' * x'))';\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Feedforward computation \n",
    "\n",
    "As a first step we implement the feedforward computation that computes $ h_\\theta(x^{(i)}) $ for every example $ i $ and sums the cost over all examples. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Backpropagation\n",
    "\n",
    "As a second step, we will implement the backpropagation algorithm to compute the gradients \n",
    "$ \\frac{\\partial}{\\partial \\theta_{ij}^{(l)}} J(\\Theta) $ of our cost function. \n",
    "\n",
    "Recall that the intuition behind the backpropagation algorithm is as follows. Given a\n",
    "training example $ (x^{(t)}, y^{(t)}) $, we will first run a “forward pass” to compute\n",
    "all the activations throughout the network, including the output value of the\n",
    "hypothesis $ h_\\theta (x{(t}) $. \n",
    "\n",
    "Then, for each node j in layer l, we would like to compute\n",
    "an “error term” $ \\delta_j(l) $ that measures how much that node was “responsible” \n",
    "for any errors in our output.\n",
    "\n",
    "* For an output node, we can directly measure the difference between the\n",
    "network’s activation and the true target value, and use that to define $ \\delta_j^{(3)} $\n",
    "(since layer 3 is the output layer):\n",
    "\n",
    "$$ \\delta_j^{(3)} = \\alpha_j^{(3)} - y_j $$\n",
    "\n",
    "* For the hidden units, we can compute $ δ_{j}(l) $ based on a weighted average of the error terms \n",
    "$ \\delta^{(l+1)} $ of the nodes in layer $ (l + 1) $:\n",
    "\n",
    "$$ \\delta^{(l)} = {\\Theta^{(l)}}^T \\delta_{(l+1)} .* g'(z^{(l)}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why? Let's assume a very simple, *linear* neural network. \n",
    "![Backpropagation Intuition](backprop_intuition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$ \\delta^{(l)} $ can be seen as the change of the network's cost function $ J $ in relation to a change in the output $ z^{(l)} $ of our node in layer $ l $.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\delta^{(l)} & = \\frac{\\partial}{\\partial z^{(l)}} J(\\theta^{l}) \\\\\n",
    "    & = \\frac{\\partial J(\\theta^{l})}{\\partial z^{(l)}} \\frac{\\partial z^{(l+1)}}{\\partial z^{(l+1)}} \n",
    "    = \\underbrace{\\frac{\\partial J(\\theta^{l})}{\\partial z^{(l+1)}}}_{= \\delta^{(l+1)}} \\underbrace{\\frac{\\partial z^{(l+1)}}{\\partial z^{(l)}}}_{=(*)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let's calculate (*):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial z^{(l+1)}}{\\partial z^{(l)}} & \\overbrace{=}^{k:=l+1} \\frac{\\partial z^{(k)}}{\\partial z^{(k-1)}} \n",
    "     = \\frac{\\partial}{\\partial z^{(k-1)}}(\\theta^{(k-1)} g(z^{(k-1)})) \n",
    "     = \\theta^{(k-1)} g'(z^{(k-1)}) \\\\\n",
    "     & \\overbrace{=}^{l=k-1} = \\theta^{(l)} g'(z^{(l)}) \\\\ \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Put together:\n",
    "\n",
    "$$ \\delta^{(l)} = \\delta^{(l+1)} \\theta^{(l)} g'(z^{(l)}) $$\n",
    "\n",
    "Recall, recall that \n",
    "\n",
    "$$ g'(z^{(l)}) = g(z^{(l)})(1-g(z^{(l)})) = \\alpha^{l}(1-\\alpha^{(l)}) $$ \n",
    "\n",
    "With\n",
    "\n",
    "$$ \\Delta^{(l)} = \\delta^{(l+1)} (\\alpha^{(l)})^T $$\n",
    "\n",
    "we can compute our gradients as follows:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_{ij}^{(l)}} J(\\Theta) = \\frac{1}{m} \\Delta_{ij}^{(l)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Regularized cost function\n",
    "\n",
    "For neural networks *with regularization* we add a regularization term to the cost function.\n",
    "\n",
    "$$ r = \\frac{\\lambda}{m} ( \\sum_{\\Theta^{(1)}_{j \\neq 1, i}} (\\Theta^{1}_{j, i})^2 ) +  \\sum_{\\Theta^{(2)}_{j \\neq 1, i \\neq 1}} (\\Theta^{2}_{j, i})^2 $$\n",
    "\n",
    "Note that you should not be regularizing the terms that correspond to the bias. For the matrices `Theta1` and `Theta2`, this corresponds to the first column of each matrix.\n",
    "\n",
    "For the gradients, this means, that we have to add an additional regularization of $ \\frac{\\lambda}{m} \\Theta_{ji}^{(l)} , j > 1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function [J grad] = nnCostFunction(nn_params, ...\n",
    "                                   input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, ...\n",
    "                                   X, y, lambda)\n",
    "%NNCOSTFUNCTION Implements the neural network cost function for a two layer\n",
    "%neural network which performs classification\n",
    "%   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...\n",
    "%   X, y, lambda) computes the cost and gradient of the neural network. The\n",
    "%   parameters for the neural network are \"unrolled\" into the vector\n",
    "%   nn_params and need to be converted back into the weight matrices. \n",
    "% \n",
    "%   The returned parameter grad should be a \"unrolled\" vector of the\n",
    "%   partial derivatives of the neural network.\n",
    "%\n",
    "\n",
    "    % Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    % for our 2 layer neural network\n",
    "    Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "    Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "    % dim_Theta1 = size(Theta1)\n",
    "    % dim_Theta2 = size(Theta2)\n",
    "    \n",
    "    % Setup some useful variables\n",
    "    m = size(X, 1)\n",
    "         \n",
    "    % We need to return the following variables correctly \n",
    "    J = 0;\n",
    "    Theta1_grad = zeros(size(Theta1));\n",
    "    Theta2_grad = zeros(size(Theta2));\n",
    "\n",
    "    % Part 1: Feedforward the neural network and return the cost in the\n",
    "    %         variable J. \n",
    "    \n",
    "    % Add additional bias node to X\n",
    "    X = [ones(m, 1) X];\n",
    "    \n",
    "    % Note, that whereas the original labels (in the variable y) were 1, 2, ..., 10, \n",
    "    % for the purpose of training a neural network, \n",
    "    % we need to recode the labels as vectors containing only values 0 or 1. \n",
    "    y_Vec = zeros(m, num_labels);\n",
    "    for i =1:m\n",
    "        y_Vec(i, y(i)) = 1;\n",
    "    end\n",
    "\n",
    "    % Theta1, Theta2 need to be transposed, since h(theta, X) expects theta to be a column vector\n",
    "    % In Theta1, Theta2 however, the parameters for each node are represented as a row\n",
    "\n",
    "    % Hidden layer\n",
    "    alpha2 = h(Theta1', X);\n",
    "    \n",
    "    % Add additional bias node alpha2(0)\n",
    "    alpha2 = [ones(m, 1) alpha2];\n",
    "    \n",
    "    % Output layer\n",
    "    alpha3 = h(Theta2', alpha2);\n",
    "    \n",
    "    % Cost function without regularization term\n",
    "\n",
    "    % We can use matrix multiplication to compute our inner sum\n",
    "    inner = -log(alpha3)*y_Vec' - log(1-alpha3)*(1-y_Vec') ;\n",
    "    sum1 = sum(diag(inner));\n",
    "    \n",
    "    J = 1/m * sum1; \n",
    "\n",
    "\n",
    "    % Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "    %         Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "    %         the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "    %         Theta2_grad, respectively. \n",
    "\n",
    "    % Output layer\n",
    "    delta3 = alpha3 - y_Vec;\n",
    "    \n",
    "    % Hidden layer\n",
    "    delta2 = delta3 * Theta2 .* alpha2 .* (1-alpha2);\n",
    "    delta2 = delta2(:, 2:end);\n",
    "    \n",
    "    Delta1 = delta2' * X;\n",
    "    Delta2 = delta3' * alpha2;\n",
    "    \n",
    "    Theta1_grad = 1/m * Delta1;\n",
    "    Theta2_grad = 1/m * Delta2;\n",
    "\n",
    "    %\n",
    "    % Part 3: Implement regularization with the cost function and gradients.\n",
    "    %    \n",
    "    \n",
    "    % Regularization term for the cost function\n",
    "    Theta1_squared = Theta1 .^ 2;\n",
    "    Theta2_squared = Theta2 .^ 2;\n",
    "    \n",
    "    Theta1_squared = Theta1_squared(:, 2:end);\n",
    "    Theta2_squared = Theta2_squared(:, 2:end);\n",
    "    \n",
    "    reg_sum = sum(Theta1_squared(:)) + sum(Theta2_squared(:));\n",
    "    J = J + lambda /(2*m) * reg_sum;\n",
    "\n",
    "    % Regularization terms for the gradient matrices\n",
    "    R1 = Theta1;\n",
    "    R1(:, 1) = 0;\n",
    "    R2 = Theta2;\n",
    "    R2(:, 1) = 0;\n",
    "\n",
    "    Theta1_grad = Theta1_grad + lambda/m * R1;\n",
    "    Theta2_grad = Theta2_grad + lambda/m * R2;\n",
    "    \n",
    "    % Unroll gradients\n",
    "    grad = [Theta1_grad(:) ; Theta2_grad(:)];\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m =  3\n",
      "J =  7.4070\n",
      "grad =\n",
      "\n",
      "   0.766138\n",
      "   0.979897\n",
      "  -0.027540\n",
      "  -0.035844\n",
      "  -0.024929\n",
      "  -0.053862\n",
      "   0.883417\n",
      "   0.568762\n",
      "   0.584668\n",
      "   0.598139\n",
      "   0.459314\n",
      "   0.344618\n",
      "   0.256313\n",
      "   0.311885\n",
      "   0.478337\n",
      "   0.368920\n",
      "   0.259771\n",
      "   0.322331\n",
      "\n",
      "m =  3\n",
      "J =  19.474\n",
      "grad =\n",
      "\n",
      "   0.76614\n",
      "   0.97990\n",
      "   0.37246\n",
      "   0.49749\n",
      "   0.64174\n",
      "   0.74614\n",
      "   0.88342\n",
      "   0.56876\n",
      "   0.58467\n",
      "   0.59814\n",
      "   1.92598\n",
      "   1.94462\n",
      "   1.98965\n",
      "   2.17855\n",
      "   2.47834\n",
      "   2.50225\n",
      "   2.52644\n",
      "   2.72233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% Addional testcase\n",
    "il = 2;              % input layer\n",
    "hl = 2;              % hidden layer\n",
    "nl = 4;              % number of labels\n",
    "nn = [ 1:18 ] / 10;  % nn_params\n",
    "X_test = cos([1 2 ; 3 4 ; 5 6]);\n",
    "y_test = [4; 2; 3];\n",
    "[J grad] = nnCostFunction(nn, il, hl, nl, X_test, y_test, 0)\n",
    "[J grad] = nnCostFunction(nn, il, hl, nl, X_test, y_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m =  5000\n",
      "Cost at parameters (loaded from ex4weights): 0.287629 \n",
      "(this value should be about 0.287629)\n",
      "m =  5000\n",
      "Cost at parameters (loaded from ex4weights) with regularization (lambda=1): 0.383770 \n",
      "(this value should be about 0.383770)\n"
     ]
    }
   ],
   "source": [
    "% Unroll parameters \n",
    "nn_params = [Theta1(:) ; Theta2(:)];\n",
    "\n",
    "% Call cost function\n",
    "% without regularization:\n",
    "lambda = 0;\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from ex4weights): %f '...\n",
    "         '\\n(this value should be about 0.287629)\\n'], J);\n",
    "         \n",
    "% with regularization (lambda = 1):\n",
    "lambda = 1;\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from ex4weights) with regularization (lambda=1): %f '...\n",
    "         '\\n(this value should be about 0.383770)\\n'], J);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "\n",
    "We can apply a method called *gradient checking* to verify numerically, if the computation of the gradients through backpropagation was correct. \n",
    "\n",
    "The idea ist to \"onroll\" $ \\Theta^{(1)}, \\Theta^{(2)} $ into a long vector $ \\theta $ and work with a function $ J{\\theta) $.\n",
    "\n",
    "We can now verify if our \"onrolled\" gradient $ f_i(\\theta) $ computed above matches a numerical approximation of the gradient for each $ i $:\n",
    "\n",
    "$$ f_i(\\theta) = \\frac {J(\\theta^{(i+)}) - J(\\theta^{(i-)}) } {2 \\epsilon} $$\n",
    "\n",
    "$ \\theta^{(i+)} $ is the same as $ \\theta $, except its $i$th element has been incremented by $ \\epsilon $. Similarly, $ \\theta^{(i-)} $ is the corresponding vector with the $i$th element decreased by $ \\epsilon $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function W = debugInitializeWeights(fan_out, fan_in)\n",
    "%DEBUGINITIALIZEWEIGHTS Initialize the weights of a layer with fan_in\n",
    "%incoming connections and fan_out outgoing connections using a fixed\n",
    "%strategy, this will help you later in debugging\n",
    "%   W = DEBUGINITIALIZEWEIGHTS(fan_in, fan_out) initializes the weights \n",
    "%   of a layer with fan_in incoming connections and fan_out outgoing \n",
    "%   connections using a fix set of values\n",
    "%\n",
    "%   Note that W should be set to a matrix of size(1 + fan_in, fan_out) as\n",
    "%   the first row of W handles the \"bias\" terms\n",
    "%\n",
    "\n",
    "    % Set W to zeros\n",
    "    W = zeros(fan_out, 1 + fan_in);\n",
    "\n",
    "    % Initialize W using \"sin\", this ensures that W is always of the same\n",
    "    % values and will be useful for debugging\n",
    "    W = reshape(sin(1:numel(W)), size(W)) / 10;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function numgrad = computeNumericalGradient(J, theta)\n",
    "%COMPUTENUMERICALGRADIENT Computes the gradient using \"finite differences\"\n",
    "%and gives us a numerical estimate of the gradient.\n",
    "%   numgrad = COMPUTENUMERICALGRADIENT(J, theta) computes the numerical\n",
    "%   gradient of the function J around theta. Calling y = J(theta) should\n",
    "%   return the function value at theta.\n",
    "\n",
    "% Notes: The following code implements numerical gradient checking, and \n",
    "%        returns the numerical gradient.It sets numgrad(i) to (a numerical \n",
    "%        approximation of) the partial derivative of J with respect to the \n",
    "%        i-th input argument, evaluated at theta. (i.e., numgrad(i) should \n",
    "%        be the (approximately) the partial derivative of J with respect \n",
    "%        to theta(i).)\n",
    "%                \n",
    "\n",
    "    numgrad = zeros(size(theta));\n",
    "    perturb = zeros(size(theta));\n",
    "    e = 1e-4;\n",
    "    for p = 1:numel(theta)\n",
    "        % Set perturbation vector\n",
    "        perturb(p) = e;\n",
    "        loss1 = J(theta - perturb);\n",
    "        loss2 = J(theta + perturb);\n",
    "        % Compute Numerical Gradient\n",
    "        numgrad(p) = (loss2 - loss1) / (2*e);\n",
    "        perturb(p) = 0;\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "function checkNNGradients(lambda)\n",
    "%CHECKNNGRADIENTS Creates a small neural network to check the\n",
    "%backpropagation gradients\n",
    "%   CHECKNNGRADIENTS(lambda) Creates a small neural network to check the\n",
    "%   backpropagation gradients, it will output the analytical gradients\n",
    "%   produced by your backprop code and the numerical gradients (computed\n",
    "%   using computeNumericalGradient). These two gradient computations should\n",
    "%   result in very similar values.\n",
    "%\n",
    "\n",
    "    if ~exist('lambda', 'var') || isempty(lambda)\n",
    "        lambda = 0;\n",
    "    end\n",
    "\n",
    "    input_layer_size = 3;\n",
    "    hidden_layer_size = 5;\n",
    "    num_labels = 3;\n",
    "    m = 5;\n",
    "\n",
    "    % We generate some 'random' test data\n",
    "    Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size);\n",
    "    Theta2 = debugInitializeWeights(num_labels, hidden_layer_size);\n",
    "    % Reusing debugInitializeWeights to generate X\n",
    "    X  = debugInitializeWeights(m, input_layer_size - 1);\n",
    "    y  = 1 + mod(1:m, num_labels)';\n",
    "\n",
    "    % Unroll parameters\n",
    "    nn_params = [Theta1(:) ; Theta2(:)];\n",
    "\n",
    "    % Short hand for cost function\n",
    "    costFunc = @(p) nnCostFunction(p, input_layer_size, hidden_layer_size, ...\n",
    "                               num_labels, X, y, lambda);\n",
    "\n",
    "    [cost, grad] = costFunc(nn_params);\n",
    "    numgrad = computeNumericalGradient(costFunc, nn_params);\n",
    "\n",
    "    % Visually examine the two gradient computations.  The two columns\n",
    "    % you get should be very similar. \n",
    "    disp([numgrad grad]);\n",
    "    fprintf(['The above two columns you get should be very similar.\\n' ...\n",
    "         '(Left-Your Numerical Gradient, Right-Analytical Gradient)\\n\\n']);\n",
    "\n",
    "    % Evaluate the norm of the difference between two solutions.  \n",
    "    % If you have a correct implementation, and assuming you used EPSILON = 0.0001 \n",
    "    % in computeNumericalGradient.m, then diff below should be less than 1e-9\n",
    "    diff = norm(numgrad-grad)/norm(numgrad+grad);\n",
    "\n",
    "    fprintf(['If your backpropagation implementation is correct, then \\n' ...\n",
    "         'the relative difference will be small (less than 1e-9). \\n' ...\n",
    "         '\\nRelative Difference: %g\\n'], diff);\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "  -0.0092782523  -0.0092782524\n",
      "   0.0088991196   0.0088991196\n",
      "  -0.0083601076  -0.0083601076\n",
      "   0.0076281355   0.0076281355\n",
      "  -0.0067479837  -0.0067479837\n",
      "  -0.0000030498  -0.0000030498\n",
      "   0.0000142869   0.0000142869\n",
      "  -0.0000259383  -0.0000259383\n",
      "   0.0000369883   0.0000369883\n",
      "  -0.0000468760  -0.0000468760\n",
      "  -0.0001750601  -0.0001750601\n",
      "   0.0002331464   0.0002331464\n",
      "  -0.0002874687  -0.0002874687\n",
      "   0.0003353203   0.0003353203\n",
      "  -0.0003762156  -0.0003762156\n",
      "  -0.0000962661  -0.0000962661\n",
      "   0.0001179827   0.0001179827\n",
      "  -0.0001371497  -0.0001371497\n",
      "   0.0001532471   0.0001532471\n",
      "  -0.0001665603  -0.0001665603\n",
      "   0.3145449700   0.3145449701\n",
      "   0.1110565882   0.1110565882\n",
      "   0.0974006970   0.0974006970\n",
      "   0.1640908188   0.1640908188\n",
      "   0.0575736494   0.0575736493\n",
      "   0.0504575855   0.0504575855\n",
      "   0.1645679323   0.1645679323\n",
      "   0.0577867379   0.0577867378\n",
      "   0.0507530173   0.0507530173\n",
      "   0.1583393339   0.1583393339\n",
      "   0.0559235296   0.0559235296\n",
      "   0.0491620841   0.0491620841\n",
      "   0.1511275275   0.1511275275\n",
      "   0.0536967009   0.0536967009\n",
      "   0.0471456249   0.0471456249\n",
      "   0.1495683347   0.1495683347\n",
      "   0.0531542052   0.0531542052\n",
      "   0.0465597186   0.0465597186\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "\n",
      "Relative Difference: 2.25001e-11\n"
     ]
    }
   ],
   "source": [
    "checkNNGradients(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "When training neural networks, it is important to randomly initialize the parameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $ \\Theta^{(l)} $ uniformly in the range $ [-\\epsilon, +\\epsilon] $.\n",
    "\n",
    "The training done again by using the well know optimization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "function W = randInitializeWeights(L_in, L_out)\n",
    "%RANDINITIALIZEWEIGHTS Randomly initialize the weights of a layer with L_in\n",
    "%incoming connections and L_out outgoing connections\n",
    "%   W = RANDINITIALIZEWEIGHTS(L_in, L_out) randomly initializes the weights \n",
    "%   of a layer with L_in incoming connections and L_out outgoing \n",
    "%   connections. \n",
    "%\n",
    "%   Note that W should be set to a matrix of size(L_out, 1 + L_in) as\n",
    "%   the first column of W handles the \"bias\" terms\n",
    "%\n",
    "    \n",
    "    W = zeros(L_out, 1 + L_in);\n",
    "    \n",
    "    % Randomly initialize the weights to small values\n",
    "    eps = 0.12;\n",
    "    W = rand(L_out, 1 + L_in) * 2 * eps - eps;\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m =  5000\n",
      "m =  5000\n",
      "m =  5000\n",
      "m =  5000\n",
      "m =  5000     1 | Cost: 3.297939e+00\n",
      "m =  5000\n",
      "m =  5000\n",
      "m =  5000     2 | Cost: 3.254918e+00\n",
      "m =  5000\n",
      "m =  5000     3 | Cost: 3.221777e+00\n",
      "m =  5000\n",
      "m =  5000     4 | Cost: 3.118353e+00\n",
      "m =  5000\n",
      "m =  5000     5 | Cost: 2.871056e+00\n",
      "m =  5000\n",
      "m =  5000     6 | Cost: 2.384246e+00\n",
      "m =  5000     7 | Cost: 2.039487e+00\n",
      "m =  5000\n",
      "m =  5000     8 | Cost: 1.945045e+00\n",
      "m =  5000     9 | Cost: 1.826975e+00\n",
      "m =  5000    10 | Cost: 1.768237e+00\n",
      "m =  5000    11 | Cost: 1.650729e+00\n",
      "m =  5000    12 | Cost: 1.546114e+00\n",
      "m =  5000\n",
      "m =  5000    13 | Cost: 1.514566e+00\n",
      "m =  5000\n",
      "m =  5000    14 | Cost: 1.394788e+00\n",
      "m =  5000\n",
      "m =  5000    15 | Cost: 1.163227e+00\n",
      "m =  5000    16 | Cost: 1.087384e+00\n",
      "m =  5000\n",
      "m =  5000    17 | Cost: 1.050586e+00\n",
      "m =  5000    18 | Cost: 9.678755e-01\n",
      "m =  5000    19 | Cost: 8.928639e-01\n",
      "m =  5000    20 | Cost: 8.257586e-01\n",
      "m =  5000    21 | Cost: 7.821587e-01\n",
      "m =  5000\n",
      "m =  5000    22 | Cost: 7.643840e-01\n",
      "m =  5000    23 | Cost: 7.426642e-01\n",
      "m =  5000    24 | Cost: 7.263109e-01\n",
      "m =  5000    25 | Cost: 7.109944e-01\n",
      "m =  5000    26 | Cost: 7.014560e-01\n",
      "m =  5000    27 | Cost: 6.904134e-01\n",
      "m =  5000    28 | Cost: 6.646547e-01\n",
      "m =  5000    29 | Cost: 6.455411e-01\n",
      "m =  5000    30 | Cost: 6.310349e-01\n",
      "m =  5000\n",
      "m =  5000    31 | Cost: 6.221766e-01\n",
      "m =  5000    32 | Cost: 6.145356e-01\n",
      "m =  5000\n",
      "m =  5000    33 | Cost: 6.092355e-01\n",
      "m =  5000    34 | Cost: 6.040427e-01\n",
      "m =  5000    35 | Cost: 5.998618e-01\n",
      "m =  5000\n",
      "m =  5000    36 | Cost: 5.878160e-01\n",
      "m =  5000    37 | Cost: 5.715380e-01\n",
      "m =  5000    38 | Cost: 5.613109e-01\n",
      "m =  5000\n",
      "m =  5000    39 | Cost: 5.536140e-01\n",
      "m =  5000    40 | Cost: 5.442903e-01\n",
      "m =  5000\n",
      "m =  5000    41 | Cost: 5.411204e-01\n",
      "m =  5000\n",
      "m =  5000    42 | Cost: 5.393187e-01\n",
      "m =  5000\n",
      "m =  5000    43 | Cost: 5.334700e-01\n",
      "m =  5000\n",
      "m =  5000    44 | Cost: 5.181383e-01\n",
      "m =  5000    45 | Cost: 4.979996e-01\n",
      "m =  5000    46 | Cost: 4.874777e-01\n",
      "m =  5000\n",
      "m =  5000    47 | Cost: 4.842996e-01\n",
      "m =  5000\n",
      "m =  5000    48 | Cost: 4.734155e-01\n",
      "m =  5000\n",
      "m =  5000    49 | Cost: 4.694667e-01\n",
      "Iteration    50 | Cost: 4.659794e-01\n"
     ]
    }
   ],
   "source": [
    "%  After you have completed the assignment, change the MaxIter to a larger\n",
    "%  value to see how more training helps.\n",
    "options = optimset('MaxIter', 50);\n",
    "\n",
    "%  You should also try different values of lambda\n",
    "lambda = 1;\n",
    "\n",
    "% Initial parameters\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "% Unroll parameters\n",
    "initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];\n",
    "\n",
    "% Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = @(p) nnCostFunction(p, ...\n",
    "                                   input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, X, y, lambda);\n",
    "\n",
    "% Now, costFunction is a function that takes in only one argument (the\n",
    "% neural network parameters)\n",
    "[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);\n",
    "\n",
    "% Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a function that uses the neural network defined by the parameters $ (\\Theta^{(1)},\\Theta^{(2)}) $ to predict the digits for a given data set of our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "function p = predict(Theta1, Theta2, X)\n",
    "%PREDICT Predict the label of an input given a trained neural network\n",
    "%   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the\n",
    "%   trained weights of a neural network (Theta1, Theta2)\n",
    "\n",
    "    % Useful values\n",
    "    m = size(X, 1);\n",
    "    num_labels = size(Theta2, 1);\n",
    "\n",
    "    p = zeros(size(X, 1), 1);\n",
    "    \n",
    "    % Add ones to the X data matrix\n",
    "    X = [ones(m, 1) X];\n",
    "\n",
    "    % Theta1, Theta2 need to be transposed, since h(theta, X) expects theta to be a column vector\n",
    "    % In Theta1, Theta2 however, the parameters for each node are represented as a row\n",
    "\n",
    "    % Hidden layer\n",
    "    alpha2 = h(Theta1', X);\n",
    "    \n",
    "    % Add additional bias node alpha2(0)\n",
    "    alpha2 = [ones(m, 1) alpha2];\n",
    "    \n",
    "    % Output layer\n",
    "    alpha3 = h(Theta2', alpha2);\n",
    "    \n",
    "    % Pick the best output und use it as label\n",
    "    [M, p] = max(alpha3, [], 2);\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Accuracy: 95.620000\n"
     ]
    }
   ],
   "source": [
    "pred = predict(Theta1, Theta2, X);\n",
    "\n",
    "fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our trained logistic regression function is doing for our randomly selected examples from the beginning of this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayData(sel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Predict the labels for the selection and convert label 10 to 0\n",
    "guessed_numbers = mod(predict(Theta1, Theta2, sel), 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans =\n",
      "\n",
      "   0   3   0   8   7   3   6   8   7   2\n",
      "   2   4   2   9   7   8   6   8   4   2\n",
      "   0   5   0   4   9   8   9   9   7   0\n",
      "   2   6   2   5   0   8   7   1   7   3\n",
      "   9   0   3   4   7   4   8   5   5   4\n",
      "   9   3   3   1   7   5   7   2   0   7\n",
      "   0   1   2   4   8   7   4   1   1   4\n",
      "   1   7   6   8   5   0   6   5   0   9\n",
      "   0   9   7   6   7   1   7   2   9   3\n",
      "   0   6   7   2   1   1   1   3   8   2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% ... and print them as a matrix \n",
    "reshape(guessed_numbers, [10,10])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
