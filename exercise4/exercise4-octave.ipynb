{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Neural Networks Learning\n",
    "\n",
    "In this exercise, you will implement the backpropagation algorithm for neural networks and apply it to the task of hand-written digit recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dataset\n",
    "\n",
    "We are given a data set in `ex4data1.mat` that contains 5000 training examples of handwritten digits. (This is exactly the same data set as in last week's exercise 3).\n",
    "\n",
    "Each training example is a 20 pixel by 20 pixel grayscale image of the digit. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels is “unrolled” into a 400-dimensional vector. Each of these training examples becomes a single row in our data matrix X. This gives us a 5000 by 400 matrix X where every row is a training example for a handwritten digit image.\n",
    "\n",
    "The second part of the training set is a 5000-dimensional vector y that contains labels for the training set. Like in the last exercise, a “0” digit is labeled as “10”, while the digits “1” to “9” are labeled as “1” to “9” in their natural order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Load saved matrices for X and y from file\n",
    "load('data/ex4data1.mat');\n",
    "% The matrices X and y will now be in our Octave environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network is shown in the picture below. It has 3 layers – an input layer, a hidden layer and an output layer. Recall that our inputs are pixel values of digit images. Since the images are of size 20 × 20, this gives us 400 input layer units (not counting the extra bias unit which always outputs +1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Structure fo our Neural Network](network_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by visualizing a subset of the training set. We reuse the 'displayDat'a function from the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = size(X, 1);\n",
    "\n",
    "% Randomly select 100 data points \n",
    "rand_indices = randperm(m);\n",
    "sel = X(rand_indices(1:100), :);\n",
    "% and display them\n",
    "displayData(sel);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to get started, we have been provided with a set of network parameters $ (\\Theta^{(1)},\\Theta^{(2)}) $ that have neen previously trained. These are stored in ex4weights.mat. The parameters have dimensions that are sized for a neural network with 25 units in the second layer and 10 output units (corresponding to the 10 digit classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Load trained parameters as matrices Theta1 and Theta2 \n",
    "load('data/ex4weights.mat');\n",
    "% Theta1 has size 25 x 401\n",
    "% Theta2 has size 10 x 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400;  % 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;   % 25 hidden units\n",
    "num_labels = 10;          % 10 labels, from 1 to 10   \n",
    "                          % (note that we have mapped \"0\" to label 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation and cost function\n",
    "\n",
    "As a first step, we will implement the cost function and the gradient for the neural network. The cost function (without regularization) is defined as:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K}[-y_k^{(i)} \\log(h_\\theta(x^{(i)})_k) - (1-y_k^{(i)}) \\log(1 - h_\\theta(x^{(i)})_k) ] $$ \n",
    "\n",
    "where $ K $ is the number of labels (or outputs, in our case $ K = 10$ and $ h_\\theta(x^{(i)})_k = a^{(3)}_k $ is the activation value of the $k$th unit in the output layer (compare with neural network layout depicted above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always we make use of a couple of helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function g = sigmoid(z)\n",
    "%SIGMOID Compute sigmoid function\n",
    "%   g = SIGMOID(z) computes the sigmoid of z.\n",
    "\n",
    "    % The function should work on scalar *and* matrix values \n",
    "    g = zeros(size(z));\n",
    "    \n",
    "    g = 1 ./ ( 1 + exp(-z));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function g = sigmoidGradient(z)\n",
    "%SIGMOIDGRADIENT returns the gradient of the sigmoid function\n",
    "%evaluated at z\n",
    "%   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function\n",
    "%   evaluated at z. t.\n",
    "\n",
    "    % The function should work on scalar *and* matrix values\n",
    "    g = zeros(size(z));\n",
    "\n",
    "    g = sigmoid(z) .* (1-sigmoid(z));\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n",
      "  \n",
      "g =\n",
      "\n",
      "   0.19661   0.23500   0.25000   0.23500   0.19661\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "fprintf('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ');\n",
    "g = sigmoidGradient([-1 -0.5 0 0.5 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "function result = h(theta, x)\n",
    "    result = (sigmoid(theta' * x'))';\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Feedforward computation \n",
    "\n",
    "As a first step we implement the feedforward computation that computes $ h_\\theta(x^{(i)}) $ for every example $ i $ and sums the cost over all examples. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Backpropagation\n",
    "\n",
    "As a second step, we will implement the backpropagation algorithm to compute the gradients \n",
    "$ \\frac{\\partial}{\\partial \\theta_{ij}^{(l)}} J(\\Theta) $ of our cost function. \n",
    "\n",
    "Recall that the intuition behind the backpropagation algorithm is as follows. Given a\n",
    "training example $ (x^{(t)}, y^{(t)}) $, we will first run a “forward pass” to compute\n",
    "all the activations throughout the network, including the output value of the\n",
    "hypothesis $ h_\\theta (x{(t}) $. \n",
    "\n",
    "Then, for each node j in layer l, we would like to compute\n",
    "an “error term” $ \\delta_j(l) $ that measures how much that node was “responsible” \n",
    "for any errors in our output.\n",
    "\n",
    "* For an output node, we can directly measure the difference between the\n",
    "network’s activation and the true target value, and use that to define $ \\delta_j^{(3)} $\n",
    "(since layer 3 is the output layer):\n",
    "\n",
    "$$ \\delta_j^{(3)} = \\alpha_j^{(3)} - y_j $$\n",
    "\n",
    "* For the hidden units, we can compute $ δ_{j}(l) $ based on a weighted average of the error terms \n",
    "$ \\delta^{(l+1)} $ of the nodes in layer $ (l + 1) $:\n",
    "\n",
    "$$ \\delta^{(l)} = {\\Theta^{(l)}}^T \\delta_{(l+1)} .* g'(z^{(l)}) $$\n",
    "\n",
    "Why? Let's assume a very simple, *linear* neural network. $ \\delta^{(l)} $ can be seen as the change of the network's cost function $ J $ in relation to a change in the output $ z^{(l)} $ of our node in layer $ l $.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\delta^{(l)} & = \\frac{\\partial}{\\partial z^{(l)}} J(\\theta^{l}) \\\\\n",
    "    & = \\frac{\\partial J(\\theta^{l})}{\\partial z^{(l)}} \\frac{\\partial z^{(l+1)}}{\\partial z^{(l+1)}} \n",
    "    = \\underbrace{\\frac{\\partial J(\\theta^{l})}{\\partial z^{(l+1)}}}_{= \\delta^{(l+1)}} \\underbrace{\\frac{\\partial z^{(l+1)}}{\\partial z^{(l)}}}_{=(*)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let's calculate (*):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial z^{(l+1)}}{\\partial z^{(l)}} & \\overbrace{=}^{k:=l+1} \\frac{\\partial z^{(k)}}{\\partial z^{(k-1)}} \n",
    "     = \\frac{\\partial}{\\partial z^{(k-1)}}(\\theta^{(k-1)} g(z^{(k-1)})) \n",
    "     = \\theta^{(k-1)} g'(z^{(k-1)}) \\\\\n",
    "     & \\overbrace{=}^{l=k-1} = \\theta^{(l)} g'(z^{(l)}) \\\\ \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Put together:\n",
    "\n",
    "$$ \\delta^{(l)} = \\delta^{(l+1)} \\theta^{(l)} g'(z^{(l)}) $$\n",
    "\n",
    "Recall, recall that \n",
    "\n",
    "$$ g'(z^{(l)}) = g(z^{(l)})(1-g(z^{(l)})) = \\alpha^{l}(1-\\alpha^{(l)}) $$ \n",
    "\n",
    "With\n",
    "\n",
    "$$ \\Delta^{(l)} = \\delta^{(l+1)} (\\alpha^{(l)})^T $$\n",
    "\n",
    "we can compute our gradients as follows:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_{ij}^{(l)}} J(\\Theta) = \\frac{1}{m} \\Delta_{ij}^{(l)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Regularized cost function\n",
    "\n",
    "For neural networks *with regularization* we add a regularization term to the cost function.\n",
    "\n",
    "$$ r = \\frac{\\lambda}{m} ( \\sum_{\\Theta^{(1)}_{j \\neq 1, i \\neq 1}} (\\Theta^{1}_{j, i})^2 ) +  \\sum_{\\Theta^{(2)}_{j \\neq 1, i \\neq 1}} (\\Theta^{2}_{j, i})^2 $$\n",
    "\n",
    "Note that you should not be regularizing the terms that correspond to the bias. For the matrices `Theta1` and `Theta2`, this corresponds to the first column of each matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function [J grad] = nnCostFunction(nn_params, ...\n",
    "                                   input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, ...\n",
    "                                   X, y, lambda)\n",
    "%NNCOSTFUNCTION Implements the neural network cost function for a two layer\n",
    "%neural network which performs classification\n",
    "%   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...\n",
    "%   X, y, lambda) computes the cost and gradient of the neural network. The\n",
    "%   parameters for the neural network are \"unrolled\" into the vector\n",
    "%   nn_params and need to be converted back into the weight matrices. \n",
    "% \n",
    "%   The returned parameter grad should be a \"unrolled\" vector of the\n",
    "%   partial derivatives of the neural network.\n",
    "%\n",
    "\n",
    "    % Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    % for our 2 layer neural network\n",
    "    Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "    Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "    % dim_Theta1 = size(Theta1)\n",
    "    % dim_Theta2 = size(Theta2)\n",
    "    \n",
    "    % Setup some useful variables\n",
    "    m = size(X, 1)\n",
    "         \n",
    "    % We need to return the following variables correctly \n",
    "    J = 0;\n",
    "    Theta1_grad = zeros(size(Theta1));\n",
    "    Theta2_grad = zeros(size(Theta2));\n",
    "\n",
    "    % Part 1: Feedforward the neural network and return the cost in the\n",
    "    %         variable J. \n",
    "    \n",
    "    % Add additional bias node to X\n",
    "    X = [ones(m, 1) X];\n",
    "    \n",
    "    % Note, that whereas the original labels (in the variable y) were 1, 2, ..., 10, \n",
    "    % for the purpose of training a neural network, \n",
    "    % we need to recode the labels as vectors containing only values 0 or 1. \n",
    "    y_Vec = zeros(m, num_labels);\n",
    "    for i =1:m\n",
    "        y_Vec(i, y(i)) = 1;\n",
    "    end\n",
    "\n",
    "    % Theta1, Theta2 need to be transposed, since h(theta, X) expects theta to be a column vector\n",
    "    % In Theta1, Theta2 however, the parameters for each node are represented as a row\n",
    "\n",
    "    % Hidden layer\n",
    "    alpha2 = h(Theta1', X);\n",
    "    \n",
    "    % Add additional bias node alpha2(0)\n",
    "    alpha2 = [ones(m, 1) alpha2];\n",
    "    \n",
    "    % Output layer\n",
    "    alpha3 = h(Theta2', alpha2);\n",
    "    \n",
    "    % Cost function without regularization term\n",
    "\n",
    "    % We can use matrix multiplication to compute our inner sum\n",
    "    inner = -log(alpha3)*y_Vec' - log(1-alpha3)*(1-y_Vec') ;\n",
    "    sum1 = sum(diag(inner));\n",
    "    \n",
    "    J = 1/m * sum1; \n",
    "\n",
    "\n",
    "    % Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "    %         Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "    %         the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "    %         Theta2_grad, respectively. \n",
    "\n",
    "    % Output layer\n",
    "    delta3 = alpha3 - y_Vec;\n",
    "    \n",
    "    % Hidden layer\n",
    "    delta2 = delta3 * Theta2 .* alpha2 .* (1-alpha2);\n",
    "    delta2 = delta2(:, 2:end);\n",
    "    \n",
    "    Delta1 = delta2' * X;\n",
    "    Delta2 = delta3' * alpha2;\n",
    "    \n",
    "    Theta1_grad = 1/m * Delta1;\n",
    "    Theta2_grad = 1/m * Delta2;\n",
    "\n",
    "    %\n",
    "    % Part 3: Implement regularization with the cost function and gradients.\n",
    "    %    \n",
    "    \n",
    "    % Regularization term for the cost function\n",
    "    Theta1_squared = Theta1 .^ 2;\n",
    "    Theta2_squared = Theta2 .^ 2;\n",
    "    \n",
    "    Theta1_squared = Theta1_squared(:, 2:end);\n",
    "    Theta2_squared = Theta2_squared(:, 2:end);\n",
    "    \n",
    "    reg_sum = sum(Theta1_squared(:)) + sum(Theta2_squared(:));\n",
    "    J = J + lambda /(2*m) * reg_sum;\n",
    "\n",
    "    % Regularization terms for the gradient matrices\n",
    "    R1 = Theta1;\n",
    "    R1(:, 1) = 0;\n",
    "    R2 = Theta2;\n",
    "    R2(:, 1) = 0;\n",
    "\n",
    "    Theta1_grad = Theta1_grad + lambda/m * R1;\n",
    "    Theta2_grad = Theta2_grad + lambda/m * R2;\n",
    "    \n",
    "    % Unroll gradients\n",
    "    grad = [Theta1_grad(:) ; Theta2_grad(:)];\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m =  3\n",
      "J =  7.4070\n",
      "grad =\n",
      "\n",
      "   0.766138\n",
      "   0.979897\n",
      "  -0.027540\n",
      "  -0.035844\n",
      "  -0.024929\n",
      "  -0.053862\n",
      "   0.883417\n",
      "   0.568762\n",
      "   0.584668\n",
      "   0.598139\n",
      "   0.459314\n",
      "   0.344618\n",
      "   0.256313\n",
      "   0.311885\n",
      "   0.478337\n",
      "   0.368920\n",
      "   0.259771\n",
      "   0.322331\n",
      "\n",
      "m =  3\n",
      "J =  19.474\n",
      "grad =\n",
      "\n",
      "   0.76614\n",
      "   0.97990\n",
      "   0.37246\n",
      "   0.49749\n",
      "   0.64174\n",
      "   0.74614\n",
      "   0.88342\n",
      "   0.56876\n",
      "   0.58467\n",
      "   0.59814\n",
      "   1.92598\n",
      "   1.94462\n",
      "   1.98965\n",
      "   2.17855\n",
      "   2.47834\n",
      "   2.50225\n",
      "   2.52644\n",
      "   2.72233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% Addional testcase\n",
    "il = 2;              % input layer\n",
    "hl = 2;              % hidden layer\n",
    "nl = 4;              % number of labels\n",
    "nn = [ 1:18 ] / 10;  % nn_params\n",
    "X_test = cos([1 2 ; 3 4 ; 5 6]);\n",
    "y_test = [4; 2; 3];\n",
    "[J grad] = nnCostFunction(nn, il, hl, nl, X_test, y_test, 0)\n",
    "[J grad] = nnCostFunction(nn, il, hl, nl, X_test, y_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m =  5000\n",
      "Cost at parameters (loaded from ex4weights): 0.287629 \n",
      "(this value should be about 0.287629)\n",
      "m =  5000\n",
      "Cost at parameters (loaded from ex4weights) with regularization (lambda=1): 0.383770 \n",
      "(this value should be about 0.383770)\n"
     ]
    }
   ],
   "source": [
    "% Unroll parameters \n",
    "nn_params = [Theta1(:) ; Theta2(:)];\n",
    "\n",
    "% Call cost function\n",
    "% without regularization:\n",
    "lambda = 0;\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from ex4weights): %f '...\n",
    "         '\\n(this value should be about 0.287629)\\n'], J);\n",
    "         \n",
    "% with regularization (lambda = 1):\n",
    "lambda = 1;\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from ex4weights) with regularization (lambda=1): %f '...\n",
    "         '\\n(this value should be about 0.383770)\\n'], J);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "\n",
    "We can apply a method called *gradient checking* to verify numerically, if the computation of the gradients through backpropagation was correct. \n",
    "\n",
    "The idea ist to \"onroll\" $ \\Theta^{(1)}, \\Theta^{(2)} $ into a long vector $ \\theta $ and work with a function $ J{\\theta) $.\n",
    "\n",
    "We can now verify if our \"onrolled\" gradient $ f_i(\\theta) $ computed above matches a numerical approximation of the gradient for each $ i $:\n",
    "\n",
    "$$ f_i(\\theta) = \\frac {J(\\theta^{(i+)}) - J(\\theta^{(i-)}) } {2 \\epsilon} $$\n",
    "\n",
    "$ \\theta^{(i+)} $ is the same as $ \\theta $, except its $i$th element has been incremented by $ \\epsilon $. Similarly, $ \\theta^{(i-)} $ is the corresponding vector with the $i$th element decreased by $ \\epsilon $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function W = debugInitializeWeights(fan_out, fan_in)\n",
    "%DEBUGINITIALIZEWEIGHTS Initialize the weights of a layer with fan_in\n",
    "%incoming connections and fan_out outgoing connections using a fixed\n",
    "%strategy, this will help you later in debugging\n",
    "%   W = DEBUGINITIALIZEWEIGHTS(fan_in, fan_out) initializes the weights \n",
    "%   of a layer with fan_in incoming connections and fan_out outgoing \n",
    "%   connections using a fix set of values\n",
    "%\n",
    "%   Note that W should be set to a matrix of size(1 + fan_in, fan_out) as\n",
    "%   the first row of W handles the \"bias\" terms\n",
    "%\n",
    "\n",
    "    % Set W to zeros\n",
    "    W = zeros(fan_out, 1 + fan_in);\n",
    "\n",
    "    % Initialize W using \"sin\", this ensures that W is always of the same\n",
    "    % values and will be useful for debugging\n",
    "    W = reshape(sin(1:numel(W)), size(W)) / 10;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function numgrad = computeNumericalGradient(J, theta)\n",
    "%COMPUTENUMERICALGRADIENT Computes the gradient using \"finite differences\"\n",
    "%and gives us a numerical estimate of the gradient.\n",
    "%   numgrad = COMPUTENUMERICALGRADIENT(J, theta) computes the numerical\n",
    "%   gradient of the function J around theta. Calling y = J(theta) should\n",
    "%   return the function value at theta.\n",
    "\n",
    "% Notes: The following code implements numerical gradient checking, and \n",
    "%        returns the numerical gradient.It sets numgrad(i) to (a numerical \n",
    "%        approximation of) the partial derivative of J with respect to the \n",
    "%        i-th input argument, evaluated at theta. (i.e., numgrad(i) should \n",
    "%        be the (approximately) the partial derivative of J with respect \n",
    "%        to theta(i).)\n",
    "%                \n",
    "\n",
    "    numgrad = zeros(size(theta));\n",
    "    perturb = zeros(size(theta));\n",
    "    e = 1e-4;\n",
    "    for p = 1:numel(theta)\n",
    "        % Set perturbation vector\n",
    "        perturb(p) = e;\n",
    "        loss1 = J(theta - perturb);\n",
    "        loss2 = J(theta + perturb);\n",
    "        % Compute Numerical Gradient\n",
    "        numgrad(p) = (loss2 - loss1) / (2*e);\n",
    "        perturb(p) = 0;\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "function checkNNGradients(lambda)\n",
    "%CHECKNNGRADIENTS Creates a small neural network to check the\n",
    "%backpropagation gradients\n",
    "%   CHECKNNGRADIENTS(lambda) Creates a small neural network to check the\n",
    "%   backpropagation gradients, it will output the analytical gradients\n",
    "%   produced by your backprop code and the numerical gradients (computed\n",
    "%   using computeNumericalGradient). These two gradient computations should\n",
    "%   result in very similar values.\n",
    "%\n",
    "\n",
    "    if ~exist('lambda', 'var') || isempty(lambda)\n",
    "        lambda = 0;\n",
    "    end\n",
    "\n",
    "    input_layer_size = 3;\n",
    "    hidden_layer_size = 5;\n",
    "    num_labels = 3;\n",
    "    m = 5;\n",
    "\n",
    "    % We generate some 'random' test data\n",
    "    Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size);\n",
    "    Theta2 = debugInitializeWeights(num_labels, hidden_layer_size);\n",
    "    % Reusing debugInitializeWeights to generate X\n",
    "    X  = debugInitializeWeights(m, input_layer_size - 1);\n",
    "    y  = 1 + mod(1:m, num_labels)';\n",
    "\n",
    "    % Unroll parameters\n",
    "    nn_params = [Theta1(:) ; Theta2(:)];\n",
    "\n",
    "    % Short hand for cost function\n",
    "    costFunc = @(p) nnCostFunction(p, input_layer_size, hidden_layer_size, ...\n",
    "                               num_labels, X, y, lambda);\n",
    "\n",
    "    [cost, grad] = costFunc(nn_params);\n",
    "    numgrad = computeNumericalGradient(costFunc, nn_params);\n",
    "\n",
    "    % Visually examine the two gradient computations.  The two columns\n",
    "    % you get should be very similar. \n",
    "    disp([numgrad grad]);\n",
    "    fprintf(['The above two columns you get should be very similar.\\n' ...\n",
    "         '(Left-Your Numerical Gradient, Right-Analytical Gradient)\\n\\n']);\n",
    "\n",
    "    % Evaluate the norm of the difference between two solutions.  \n",
    "    % If you have a correct implementation, and assuming you used EPSILON = 0.0001 \n",
    "    % in computeNumericalGradient.m, then diff below should be less than 1e-9\n",
    "    diff = norm(numgrad-grad)/norm(numgrad+grad);\n",
    "\n",
    "    fprintf(['If your backpropagation implementation is correct, then \\n' ...\n",
    "         'the relative difference will be small (less than 1e-9). \\n' ...\n",
    "         '\\nRelative Difference: %g\\n'], diff);\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "m =  5\n",
      "  -0.0092782523  -0.0092782524\n",
      "   0.0088991196   0.0088991196\n",
      "  -0.0083601076  -0.0083601076\n",
      "   0.0076281355   0.0076281355\n",
      "  -0.0067479837  -0.0067479837\n",
      "  -0.0000030498  -0.0000030498\n",
      "   0.0000142869   0.0000142869\n",
      "  -0.0000259383  -0.0000259383\n",
      "   0.0000369883   0.0000369883\n",
      "  -0.0000468760  -0.0000468760\n",
      "  -0.0001750601  -0.0001750601\n",
      "   0.0002331464   0.0002331464\n",
      "  -0.0002874687  -0.0002874687\n",
      "   0.0003353203   0.0003353203\n",
      "  -0.0003762156  -0.0003762156\n",
      "  -0.0000962661  -0.0000962661\n",
      "   0.0001179827   0.0001179827\n",
      "  -0.0001371497  -0.0001371497\n",
      "   0.0001532471   0.0001532471\n",
      "  -0.0001665603  -0.0001665603\n",
      "   0.3145449700   0.3145449701\n",
      "   0.1110565882   0.1110565882\n",
      "   0.0974006970   0.0974006970\n",
      "   0.1640908188   0.1640908188\n",
      "   0.0575736494   0.0575736493\n",
      "   0.0504575855   0.0504575855\n",
      "   0.1645679323   0.1645679323\n",
      "   0.0577867379   0.0577867378\n",
      "   0.0507530173   0.0507530173\n",
      "   0.1583393339   0.1583393339\n",
      "   0.0559235296   0.0559235296\n",
      "   0.0491620841   0.0491620841\n",
      "   0.1511275275   0.1511275275\n",
      "   0.0536967009   0.0536967009\n",
      "   0.0471456249   0.0471456249\n",
      "   0.1495683347   0.1495683347\n",
      "   0.0531542052   0.0531542052\n",
      "   0.0465597186   0.0465597186\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "\n",
      "Relative Difference: 2.25001e-11\n"
     ]
    }
   ],
   "source": [
    "checkNNGradients(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "When training neural networks, it is important to randomly initialize the parameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $ \\Theta^{(l)} $ uniformly in the range $ [-\\epsilon, +\\epsilon] $.\n",
    "\n",
    "The training done again by using the well know optimization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "function W = randInitializeWeights(L_in, L_out)\n",
    "%RANDINITIALIZEWEIGHTS Randomly initialize the weights of a layer with L_in\n",
    "%incoming connections and L_out outgoing connections\n",
    "%   W = RANDINITIALIZEWEIGHTS(L_in, L_out) randomly initializes the weights \n",
    "%   of a layer with L_in incoming connections and L_out outgoing \n",
    "%   connections. \n",
    "%\n",
    "%   Note that W should be set to a matrix of size(L_out, 1 + L_in) as\n",
    "%   the first column of W handles the \"bias\" terms\n",
    "%\n",
    "    \n",
    "    W = zeros(L_out, 1 + L_in);\n",
    "    \n",
    "    % Randomly initialize the weights to small values\n",
    "    eps = 0.12;\n",
    "    W = rand(L_out, 1 + L_in) * 2 * eps - eps;\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m =  5000\n",
      "m =  5000\n",
      "m =  5000\n",
      "m =  5000\n",
      "m =  5000     1 | Cost: 3.313755e+00\n",
      "m =  5000\n",
      "m =  5000\n",
      "m =  5000\n",
      "m =  5000     2 | Cost: 3.247555e+00\n",
      "m =  5000\n",
      "m =  5000     3 | Cost: 3.230282e+00\n",
      "m =  5000\n",
      "m =  5000\n",
      "m =  5000\n",
      "m =  5000     4 | Cost: 2.811384e+00\n",
      "m =  5000     5 | Cost: 2.531807e+00\n",
      "m =  5000     6 | Cost: 2.191695e+00\n",
      "m =  5000\n",
      "m =  5000     7 | Cost: 2.060271e+00\n",
      "m =  5000     8 | Cost: 1.936586e+00\n",
      "m =  5000     9 | Cost: 1.770072e+00\n",
      "m =  5000\n",
      "m =  5000    10 | Cost: 1.715002e+00\n",
      "m =  5000\n",
      "m =  5000    11 | Cost: 1.579801e+00\n",
      "m =  5000    12 | Cost: 1.348370e+00\n",
      "m =  5000\n",
      "m =  5000    13 | Cost: 1.300078e+00\n",
      "m =  5000    14 | Cost: 1.270104e+00\n",
      "m =  5000    15 | Cost: 1.234539e+00\n",
      "m =  5000    16 | Cost: 1.179599e+00\n",
      "m =  5000    17 | Cost: 1.140101e+00\n",
      "m =  5000    18 | Cost: 1.094471e+00\n",
      "m =  5000    19 | Cost: 1.042942e+00\n",
      "m =  5000    20 | Cost: 1.006912e+00\n",
      "m =  5000    21 | Cost: 9.624815e-01\n",
      "m =  5000    22 | Cost: 9.014425e-01\n",
      "m =  5000\n",
      "m =  5000    23 | Cost: 8.776231e-01\n",
      "m =  5000\n",
      "m =  5000    24 | Cost: 8.652826e-01\n",
      "m =  5000    25 | Cost: 8.470017e-01\n",
      "m =  5000    26 | Cost: 8.312170e-01\n",
      "m =  5000\n",
      "m =  5000    27 | Cost: 8.024280e-01\n",
      "m =  5000\n",
      "m =  5000    28 | Cost: 7.390982e-01\n",
      "m =  5000    29 | Cost: 6.669851e-01\n",
      "m =  5000\n",
      "m =  5000    30 | Cost: 6.261442e-01\n",
      "m =  5000\n",
      "m =  5000    31 | Cost: 6.139889e-01\n",
      "m =  5000    32 | Cost: 5.994365e-01\n",
      "m =  5000    33 | Cost: 5.925658e-01\n",
      "m =  5000\n",
      "m =  5000    34 | Cost: 5.910073e-01\n",
      "m =  5000\n",
      "m =  5000\n",
      "m =  5000    35 | Cost: 5.777569e-01\n",
      "m =  5000\n",
      "m =  5000    36 | Cost: 5.693785e-01\n",
      "m =  5000\n",
      "m =  5000    37 | Cost: 5.653213e-01\n",
      "m =  5000    38 | Cost: 5.596731e-01\n",
      "m =  5000    39 | Cost: 5.559132e-01\n",
      "m =  5000    40 | Cost: 5.510945e-01\n",
      "m =  5000\n",
      "m =  5000    41 | Cost: 5.418046e-01\n",
      "m =  5000    42 | Cost: 5.342781e-01\n",
      "m =  5000    43 | Cost: 5.225820e-01\n",
      "m =  5000    44 | Cost: 5.141043e-01\n",
      "m =  5000\n",
      "m =  5000    45 | Cost: 5.089074e-01\n",
      "m =  5000    46 | Cost: 5.055791e-01\n",
      "m =  5000\n",
      "m =  5000    47 | Cost: 5.040275e-01\n",
      "m =  5000\n",
      "m =  5000    48 | Cost: 5.000837e-01\n",
      "m =  5000\n",
      "m =  5000    49 | Cost: 4.983007e-01\n",
      "Iteration    50 | Cost: 4.962783e-01\n"
     ]
    }
   ],
   "source": [
    "%  After you have completed the assignment, change the MaxIter to a larger\n",
    "%  value to see how more training helps.\n",
    "options = optimset('MaxIter', 50);\n",
    "\n",
    "%  You should also try different values of lambda\n",
    "lambda = 1;\n",
    "\n",
    "% Initial parameters\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "% Unroll parameters\n",
    "initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];\n",
    "\n",
    "% Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = @(p) nnCostFunction(p, ...\n",
    "                                   input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, X, y, lambda);\n",
    "\n",
    "% Now, costFunction is a function that takes in only one argument (the\n",
    "% neural network parameters)\n",
    "[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);\n",
    "\n",
    "% Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a function that uses the neural network defined by the parameters $ (\\Theta^{(1)},\\Theta^{(2)}) $ to predict the digits for a given data set of our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "function p = predict(Theta1, Theta2, X)\n",
    "%PREDICT Predict the label of an input given a trained neural network\n",
    "%   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the\n",
    "%   trained weights of a neural network (Theta1, Theta2)\n",
    "\n",
    "    % Useful values\n",
    "    m = size(X, 1);\n",
    "    num_labels = size(Theta2, 1);\n",
    "\n",
    "    p = zeros(size(X, 1), 1);\n",
    "    \n",
    "    % Add ones to the X data matrix\n",
    "    X = [ones(m, 1) X];\n",
    "\n",
    "    % Theta1, Theta2 need to be transposed, since h(theta, X) expects theta to be a column vector\n",
    "    % In Theta1, Theta2 however, the parameters for each node are represented as a row\n",
    "\n",
    "    % Hidden layer\n",
    "    alpha2 = h(Theta1', X);\n",
    "    \n",
    "    % Add additional bias node alpha2(0)\n",
    "    alpha2 = [ones(m, 1) alpha2];\n",
    "    \n",
    "    % Output layer\n",
    "    alpha3 = h(Theta2', alpha2);\n",
    "    \n",
    "    % Pick the best output und use it as label\n",
    "    [M, p] = max(alpha3, [], 2);\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Accuracy: 94.920000\n"
     ]
    }
   ],
   "source": [
    "pred = predict(Theta1, Theta2, X);\n",
    "\n",
    "fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our trained logistic regression function is doing for our randomly selected examples from the beginning of this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayData(sel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Predict the labels for the selection and convert label 10 to 0\n",
    "guessed_numbers = mod(predict(Theta1, Theta2, sel), 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans =\n",
      "\n",
      "   1   8   0   1   4   7   8   8   0   5\n",
      "   5   8   6   7   5   0   4   6   9   0\n",
      "   0   3   9   2   4   0   2   3   0   8\n",
      "   1   2   2   9   0   5   6   6   2   4\n",
      "   5   4   3   4   5   8   1   9   0   0\n",
      "   0   8   6   5   7   0   8   7   1   0\n",
      "   7   6   3   5   5   4   5   8   3   6\n",
      "   0   8   7   6   6   5   1   3   5   2\n",
      "   0   5   2   9   4   7   1   6   9   5\n",
      "   3   9   3   6   5   9   8   5   1   3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% ... and print them as a matrix \n",
    "reshape(guessed_numbers, [10,10])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
